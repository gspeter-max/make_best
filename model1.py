import google.generativeai as genai 
from dotenv import load_dotenv 
import os 
import json 
import sys 

class make_best: 
    
    def __init__(self,env_path = '/workspaces/make_best/api_key.env', key_name = 'api_key'):  
        try: 
            
            load_dotenv(dotenv_path= env_path) 
            self.GOOGLE_API_KEY = os.getenv(key_name) 
            genai.configure(api_key=self.GOOGLE_API_KEY)
    
        except Exception as e : 
    
            print(f"Error accessing Google API Key: {e}")
            print("Please make sure you have added GOOGLE_API_KEY to Colab Secrets (Key icon on the left).")
            self.GOOGLE_API_KEY = None # Prevent further errors
    
    def __call__(self): 
        
        model1 = make_model1() 
        model2 = make_model2() 
        while True: 
            user_prompt = input('you ;') 
            if  user_prompt.lower() == 'quit': 
                break 
            
            try : 
                model1_output = model1(user_prompt) 
                model1_output = model1_output.split('```json\n',1)[1]
                model1_output = model1_output.split('\n```',1)[0]
                try : 
                    
                    model1_output_json = json.loads(model1_output)
                    if model1_output_json.get('is_code_related',False): 
                    
                        if len(model1_output_json.get('prompt_for_model2',"").strip() ) < 2 : 
                            print(f'model1 is not give input for next model \n outupt looklike \n {model1_output_json}')
                            continue
             
                        # model1(f' that is the code generated by model2 : {model2(model1_output_json.get('prompt_for_model2'))}')
                        # print(f'gpt :- {model1_output_json.get('response_for_user')}')
                        # print(f' gpt : -- {model2(model1_output_json.get('prompt_for_model2'))}')                         model2(model1_output_json.get('prompt_for_model2'))

                    else : 
                        if len(model1_output_json.get('response_for_user','').strip()) < 2 :
                            print(f' model1 is give nothing for user_output json file is {model1_output_json}') 
                        
                        print(f'gpt :- {model1_output_json.get('response_for_user')}')
                        continue
                
                except Exception as e : 
                    raise RuntimeWarning(f'we are get error during model2 conversation look like this : {e}')
                
            
            except Exception as e : 
                raise RuntimeWarning(f'we are get error during model2 conversation look like this : {e}')
                            
        '''
            here th while logic is appling and user interface how to hand users 
            # here the call logic like 
            
            model1(user_prompt) ==> model2(model1) ==> ........... return 
            work with class and that models and that shit     
        
        '''
class make_model1(make_best): 

    def __init__(self,max_output_tokens = 7500, model_name = 'gemini-1.5-flash-latest'): 

        super().__init__() 
        genai_parameters = {
            'temperature' : 0.3, 
            'top_p' : 0.9, 
            'top_k' : 40, 
            'max_output_tokens': max_output_tokens, 
            'response_mime_type': 'text/plain'
        }
        safety_settings = [
            {
                'category' : 'HARM_CATEGORY_HARASSMENT', 
                'threshold' : 'BLOCK_MEDIUM_AND_ABOVE'
            }, 
            {
                'category' : 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 
                'threshold' :'BLOCK_MEDIUM_AND_ABOVE'
            }, 
            {
                'category' : 'HARM_CATEGORY_DANGEROUS_CONTENT', 
                'threshold' : 'BLOCK_MEDIUM_AND_ABOVE'
            }, 
            {
                'category' : 'HARM_CATEGORY_HATE_SPEECH', 
                'threshold' : 'BLOCK_MEDIUM_AND_ABOVE'

            }
        ] 

        if self.GOOGLE_API_KEY : 
            try : 
                self.model = genai.GenerativeModel(
                    model_name = model_name, 
                    safety_settings= safety_settings, 
                    generation_config = genai_parameters, 
                    system_instruction =  '''
                    
                            "You are an intelligent gatekeeper and expert prompt engineer for a specialized AI coding assistant (Model 2). "
                            "Your primary function is to analyze user input and recent conversation history to determine if a request is code-related "
                            "and then prepare the appropriate response or directive. Conversation history is crucial for context.\n\n"

                            You are an AI assistant that classifies user requests and prepares tasks for a specialized AI.
                            Your output MUST be a VALID JSON object.
                            Ensure that within JSON string values, special characters like *, -, +, _, etc., are NOT escaped with a backslash unless the backslash itself is part of a valid JSON escape sequence (like \n, \", \\). For example, use '*' directly, not '\*'.
                            The JSON schema you MUST output is:
                            {
                            "is_code_related": boolean,
                            "response_for_user": string,
                            "prompt_for_model2": string    # make sure that typo of all that keys is exactly same all the time focus on that 100% not make mistake here 
                            }

                            "YOUR BEHAVIOR:\n"
                            "1.  Analyze the user's latest input and the immediate preceding conversation context (last 3-5 turns).\n"
                            "2.  Determine if the user's request is for code generation, code modification, code optimization, or directly discusses a programming problem requiring a code solution.\n\n"
                            "3.  IF THE REQUEST IS CODE-RELATED:\n"
                            "    a.  Set `is_code_related` to `true`.\n"
                            "    b.  Set `response_for_user` to a brief acknowledgement like \"Understood. Generating efficient code for you...\" or an empty string.\n"
                            "    c.  Construct the `prompt_for_model2` field. This value MUST be a meticulously crafted, highly detailed, and directive prompt specifically FOR MODEL 2. "
                            "        This prompt must be self-contained and instruct Model 2 as follows:\n"
                            "        'You are an elite AI specializing in writing extremely efficient and comprehensive code. "
                            "        Your ONLY output should be the requested code. Do NOT include explanations, apologies, or any text other than the code itself. "
                            "        The code must be: \n"
                            "        - Maximally efficient in terms of time complexity (state and justify Big O if complex).\n"
                            "        - Maximally efficient in terms of space complexity (state and justify Big O if complex).\n"
                            "        - Thorough, covering all explicit and implicit requirements derived from the following context.\n"
                            "        - Robust, handling potential edge cases and invalid inputs gracefully.\n"
                            "        - Well-commented where non-obvious, and adhere to idiomatic style for the language.\n"
                            "        - 'Large' in the sense of being complete and well-developed, not artificially inflated.\n"
                            "        Based on this context: [INSERT DETAILED PROBLEM DESCRIPTION, CONSTRAINTS, LANGUAGE, RELEVANT HISTORY, AND USER'S EXACT REQUEST HERE. BE EXHAUSTIVE. Synthesize all information into a clear task for Model 2.]'\n"
                            "    d.  When constructing the '[INSERT...]' part for `prompt_for_model2`, synthesize the user's current request with key details from the provided conversation history (e.g., language preference, libraries mentioned, prior constraints). Be explicit and comprehensive.\n"
                            "    e.  The `prompt_for_model2` string can use as many tokens as needed, up to 7000, to ensure clarity and completeness for Model 2.\n\n"
                            "4.  ELSE (IF THE REQUEST IS NOT CODE-RELATED, or if it's ambiguous after considering history):\n"
                            "    a.  Set `is_code_related` to `false`.\n"
                            "    b.  Set `response_for_user` to a polite, conversational message (e.g., \"Nice to meet you! How can I help you with a coding task today?\" or \"I specialize in coding tasks. Is there something code-related I can assist with?\").\n"
                            "    c.  Set `prompt_for_model2` to an empty string.\n\n"

                            "Ensure your entire output is ONLY the valid JSON object described. Do not add any text before or after the JSON."
                       ''' 
                ) 
                
                self.chat_session = self.model.start_chat(history = []) 

            except Exception as e :
                raise RuntimeError('error found in initilizing model') 
            
    
    def __call__(self,user_prompt): 
    
        try:
            if not self.chat_session : 
                raise RuntimeWarning(f'warrning chat_sessions is not initilized ') 

            chunck_session = self.model.generate_content(
                contents = user_prompt, 
                stream= True
            )
            
            for chunck in chunck_session: 
                if chunck.text:
                    print(chunck.text, end = '')
                    sys.stdout.flush() 
                    
                    
            response  =  self.chat_session.send_message(user_prompt)
            return response.text 
        
        except Exception as e : 
            raise RuntimeError(f'error founded during response geting {e}') 


class make_model2(make_best): 

    def __init__(self,max_output_tokens = 8120, model_name = 'gemini-1.5-flash-latest'): 
        super().__init__() 
        
        generation_configure = {
                'temperature' : 0.3, 
                'max_output_tokens' : max_output_tokens, 
                'top_p' : 0.9,
                'top_k' : 50, 
                'response_mime_type' : 'text/plain' 
                } 

        safety_setting = [
                    {   
                        'category' : 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
                        'threshold' : 'BLOCK_MEDIUM_AND_ABOVE' 
                        },
                    {
                        'category' : 'HARM_CATEGORY_DANGEROUS_CONTENT', 
                        'threshold' : 'BLOCK_MEDIUM_AND_ABOVE'
                        }, 
                    {
                        'category' : 'HARM_CATEGORY_HATE_SPEECH', 
                        'threshold' : 'BLOCK_MEDIUM_AND_ABOVE'
                        }, 
                    {
                        'category' : 'HARM_CATEGORY_HARASSMENT', 
                        'threshold' : 'BLOCK_MEDIUM_AND_ABOVE'
                        } 
        ] 
        
        try :
            self.MODEL = genai.GenerativeModel(
                        model_name = model_name,
                        safety_settings = safety_setting, 
                        generation_config = generation_configure, 
                        system_instruction = 
                            '''
                                                    
                        **CORE DIRECTIVE: Elite AI Code Synthesis Engine**

                        **Mission Critical Objective:** Your SOLE function is to synthesize raw, executable, production-grade source code based on the precise specifications provided in the user prompt.

                        **Output Mandate: PURE CODE ONLY**
                        1.  Your response MUST consist of **ONLY the requested source code** and nothing else.
                        2.  **ABSOLUTELY NO** surrounding text, explanations, apologies, conversational remarks, introductions, summaries, or markdown code block specifiers (e.g., ```python) are permitted.
                        3.  If the request implies multiple files, structure your output logically or as directed in the prompt, but still only output the raw content of those files.

                        **Code Quality Non-Negotiables (Adherence is MANDATORY):**
                            a.  **100% Functional & Correct:** The code MUST compile/interpret and execute flawlessly, producing the correct results for all valid inputs and fulfilling all specified requirements. It must be a complete, working solution.
                            b.  **Maximal Algorithmic Efficiency:**
                                *   **Time Complexity:** Implement the most efficient algorithms possible for the given problem. If a complex algorithm is used, its Big O notation (e.g., "O(n log n) - Merge Sort") MUST be stated in a leading comment block for the relevant function/module.
                                *   **Space Complexity:** Minimize memory footprint. State space complexity in comments if significant.
                            c.  **Extreme Robustness & Reliability:**
                                *   Anticipate and gracefully handle ALL conceivable edge cases, invalid inputs (types, values, formats), and common operational failures (e.g., file I/O errors, network issues, division by zero).
                                *   Implement clear, informative, and actionable error handling and messaging.
                            d.  **Security by Design:** If the nature of the code involves data handling, user input, or external interactions, it MUST be designed with security best practices to prevent common vulnerabilities.
                            e.  **Impeccable Readability & Maintainability (Production Standard):**
                                *   Strictly adhere to idiomatic style conventions and best practices for the target language (e.g., PEP 8 for Python, Google Style Guide for Java/C++ if applicable).
                                *   Employ exceptionally clear, descriptive, and consistent naming for all identifiers (variables, functions, classes, constants).
                                *   Utilize modular design: decompose complex problems into smaller, cohesive, well-defined functions/classes with single responsibilities.
                                *   Provide concise, high-value comments ONLY to explain non-obvious logic, complex algorithms, or critical design decisions (the "why," not the "what," if the "what" is self-evident from the code). Avoid comment clutter.
                            f.  **Comprehensive & Complete:** Address all explicit and implicit requirements derived from the user's prompt. Do not omit necessary components or functionalities.
                            g.  **No Placeholders:** Deliver finished, production-ready code. ABSOLUTELY NO "TODO", "FIXME", placeholder comments, or indications of incomplete work.

                        **Operational Protocol:**
                        *   You will receive a highly detailed prompt from a preceding AI system (`model1`). This prompt contains all necessary context and precise instructions.
                        *   Your task is to transform these instructions directly into compliant source code.
                        *   **Assume the prompt is your complete and authoritative specification.** Do not infer requirements beyond it unless absolutely necessary for core functionality and safety.
                        *   If the prompt specifies a language, you MUST use it. If multiple files are implied, structure the output accordingly or as specified in the prompt (e.g., delimited by specific markers if requested by `model1`).

                        **Performance Standard:** Your output will be judged on its direct usability as raw source code and its adherence to the extreme quality standards defined above. Failure to meet these directives, especially the "PURE CODE ONLY" output mandate, is unacceptable. Synthesize with unparalleled precision and excellence.

                            ''' 
            )  
            self.chat_session = self.MODEL.start_chat(history = []) 
        
        except Exception as  e: 
            raise RuntimeError('error is found during model initilization ') 
        
    def __call__(self,user_content):
        if not self.chat_session: 
            raise RuntimeWarning('warning is found') 

        chunck_response = self.MODEL.generate_content(
            contents = user_content, 
            stream=True 
        )
        for chunck in chunck_response: 
            if chunck.text:
                print(chunck.text, end = '')
                sys.stdout.flush() 
                
        response = self.chat_session.send_message(user_content) 
        return response.text 

    
model_ = make_best() 
model_()
 



















